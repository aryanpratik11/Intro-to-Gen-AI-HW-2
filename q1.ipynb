{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Implement a Vanilla RNN from Scratch (Without TensorFlow/PyTorch)\n",
    "- Implement the forward and backward passes manually.\n",
    "- Handle weight sharing across time steps.\n",
    "- Ensure proper weight updates using gradient descent.\n",
    "- Implement Backpropagation Through Time (BPTT) manually (Advanced-optional). \n",
    "Bonus: Extend it to a multi-layer RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputSize = 5\n",
    "hidden = 5\n",
    "outputSize = 2 \n",
    "seqLen = 40\n",
    "lRate = 0.001\n",
    "epochs = 1000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [np.random.randn(inputSize, 1) for _ in range(seqLen)]\n",
    "Y = np.random.randn(outputSize, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.random.randn(hidden, inputSize) * 0.001\n",
    "w1 = np.random.randn(hidden, hidden) * 0.001\n",
    "w2= np.random.randn(outputSize, hidden) * 0.001\n",
    "b1 = np.zeros((hidden, 1))\n",
    "b2 = np.zeros((outputSize, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X):\n",
    "    hStates = []\n",
    "    hPrev = np.zeros((hidden, 1))\n",
    "\n",
    "    for t in range(seqLen):\n",
    "        hPrev = np.tanh(np.dot(w0, X[t]) + np.dot(w1, hPrev) + b1)\n",
    "        hStates.append(hPrev)\n",
    "\n",
    "    Yout = np.dot(w2, hPrev) + b2\n",
    "    return hStates, Yout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back(X, Y, hStates, Yout):\n",
    "    global w0, w1, w2, b1, b2\n",
    "\n",
    "    dw2 = np.zeros_like(w2)\n",
    "    dw0 = np.zeros_like(w0)\n",
    "    dw1 = np.zeros_like(w1)\n",
    "    db1 = np.zeros_like(b1)\n",
    "    db2 = np.zeros_like(b2)\n",
    "    dy = Yout - Y\n",
    "    dw2 += np.dot(dy, hStates[-1].T)\n",
    "    db2 += dy\n",
    "\n",
    "    dh_next = np.dot(w2.T, dy)\n",
    "\n",
    "    for t in reversed(range(seqLen)):\n",
    "        dh = (1 - hStates[t] ** 2) * dh_next \n",
    "        dw0 += np.dot(dh, X[t].T)\n",
    "        dw1 += np.dot(dh, (hStates[t - 1] if t > 0 else np.zeros((hidden, 1))).T)\n",
    "\n",
    "        db1 += dh\n",
    "        dh_next = np.dot(w1.T, dh)\n",
    "\n",
    "    return dw0, dw1, dw2, db1, db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightUpdates(dw0, dw1, dw2, db1, db2):\n",
    "    global w0, w1, w2, b1, b2 \n",
    "\n",
    "    w0 -= lRate * dw0\n",
    "    w1 -= lRate * dw1\n",
    "    w2 -= lRate * dw2\n",
    "    b1 -= lRate * db1\n",
    "    b2 -= lRate * db2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.6544\n",
      "Epoch 5, Loss: 1.6380\n",
      "Epoch 10, Loss: 1.6217\n",
      "Epoch 15, Loss: 1.6055\n",
      "Epoch 20, Loss: 1.5895\n",
      "Epoch 25, Loss: 1.5737\n",
      "Epoch 30, Loss: 1.5580\n",
      "Epoch 35, Loss: 1.5425\n",
      "Epoch 40, Loss: 1.5272\n",
      "Epoch 45, Loss: 1.5120\n",
      "Epoch 50, Loss: 1.4969\n",
      "Epoch 55, Loss: 1.4820\n",
      "Epoch 60, Loss: 1.4673\n",
      "Epoch 65, Loss: 1.4527\n",
      "Epoch 70, Loss: 1.4382\n",
      "Epoch 75, Loss: 1.4239\n",
      "Epoch 80, Loss: 1.4097\n",
      "Epoch 85, Loss: 1.3957\n",
      "Epoch 90, Loss: 1.3818\n",
      "Epoch 95, Loss: 1.3680\n",
      "Epoch 100, Loss: 1.3544\n",
      "Epoch 105, Loss: 1.3409\n",
      "Epoch 110, Loss: 1.3276\n",
      "Epoch 115, Loss: 1.3143\n",
      "Epoch 120, Loss: 1.3013\n",
      "Epoch 125, Loss: 1.2883\n",
      "Epoch 130, Loss: 1.2755\n",
      "Epoch 135, Loss: 1.2628\n",
      "Epoch 140, Loss: 1.2502\n",
      "Epoch 145, Loss: 1.2378\n",
      "Epoch 150, Loss: 1.2254\n",
      "Epoch 155, Loss: 1.2132\n",
      "Epoch 160, Loss: 1.2012\n",
      "Epoch 165, Loss: 1.1892\n",
      "Epoch 170, Loss: 1.1774\n",
      "Epoch 175, Loss: 1.1656\n",
      "Epoch 180, Loss: 1.1540\n",
      "Epoch 185, Loss: 1.1426\n",
      "Epoch 190, Loss: 1.1312\n",
      "Epoch 195, Loss: 1.1199\n",
      "Epoch 200, Loss: 1.1088\n",
      "Epoch 205, Loss: 1.0977\n",
      "Epoch 210, Loss: 1.0868\n",
      "Epoch 215, Loss: 1.0760\n",
      "Epoch 220, Loss: 1.0653\n",
      "Epoch 225, Loss: 1.0547\n",
      "Epoch 230, Loss: 1.0442\n",
      "Epoch 235, Loss: 1.0338\n",
      "Epoch 240, Loss: 1.0235\n",
      "Epoch 245, Loss: 1.0133\n",
      "Epoch 250, Loss: 1.0032\n",
      "Epoch 255, Loss: 0.9932\n",
      "Epoch 260, Loss: 0.9833\n",
      "Epoch 265, Loss: 0.9735\n",
      "Epoch 270, Loss: 0.9638\n",
      "Epoch 275, Loss: 0.9542\n",
      "Epoch 280, Loss: 0.9447\n",
      "Epoch 285, Loss: 0.9353\n",
      "Epoch 290, Loss: 0.9260\n",
      "Epoch 295, Loss: 0.9168\n",
      "Epoch 300, Loss: 0.9077\n",
      "Epoch 305, Loss: 0.8986\n",
      "Epoch 310, Loss: 0.8897\n",
      "Epoch 315, Loss: 0.8808\n",
      "Epoch 320, Loss: 0.8721\n",
      "Epoch 325, Loss: 0.8634\n",
      "Epoch 330, Loss: 0.8548\n",
      "Epoch 335, Loss: 0.8463\n",
      "Epoch 340, Loss: 0.8378\n",
      "Epoch 345, Loss: 0.8295\n",
      "Epoch 350, Loss: 0.8212\n",
      "Epoch 355, Loss: 0.8131\n",
      "Epoch 360, Loss: 0.8050\n",
      "Epoch 365, Loss: 0.7970\n",
      "Epoch 370, Loss: 0.7890\n",
      "Epoch 375, Loss: 0.7812\n",
      "Epoch 380, Loss: 0.7734\n",
      "Epoch 385, Loss: 0.7657\n",
      "Epoch 390, Loss: 0.7581\n",
      "Epoch 395, Loss: 0.7505\n",
      "Epoch 400, Loss: 0.7430\n",
      "Epoch 405, Loss: 0.7356\n",
      "Epoch 410, Loss: 0.7283\n",
      "Epoch 415, Loss: 0.7211\n",
      "Epoch 420, Loss: 0.7139\n",
      "Epoch 425, Loss: 0.7068\n",
      "Epoch 430, Loss: 0.6997\n",
      "Epoch 435, Loss: 0.6928\n",
      "Epoch 440, Loss: 0.6859\n",
      "Epoch 445, Loss: 0.6790\n",
      "Epoch 450, Loss: 0.6723\n",
      "Epoch 455, Loss: 0.6656\n",
      "Epoch 460, Loss: 0.6590\n",
      "Epoch 465, Loss: 0.6524\n",
      "Epoch 470, Loss: 0.6459\n",
      "Epoch 475, Loss: 0.6395\n",
      "Epoch 480, Loss: 0.6331\n",
      "Epoch 485, Loss: 0.6268\n",
      "Epoch 490, Loss: 0.6206\n",
      "Epoch 495, Loss: 0.6144\n",
      "Epoch 500, Loss: 0.6083\n",
      "Epoch 505, Loss: 0.6022\n",
      "Epoch 510, Loss: 0.5962\n",
      "Epoch 515, Loss: 0.5903\n",
      "Epoch 520, Loss: 0.5844\n",
      "Epoch 525, Loss: 0.5786\n",
      "Epoch 530, Loss: 0.5728\n",
      "Epoch 535, Loss: 0.5671\n",
      "Epoch 540, Loss: 0.5615\n",
      "Epoch 545, Loss: 0.5559\n",
      "Epoch 550, Loss: 0.5503\n",
      "Epoch 555, Loss: 0.5448\n",
      "Epoch 560, Loss: 0.5394\n",
      "Epoch 565, Loss: 0.5340\n",
      "Epoch 570, Loss: 0.5287\n",
      "Epoch 575, Loss: 0.5235\n",
      "Epoch 580, Loss: 0.5182\n",
      "Epoch 585, Loss: 0.5131\n",
      "Epoch 590, Loss: 0.5080\n",
      "Epoch 595, Loss: 0.5029\n",
      "Epoch 600, Loss: 0.4979\n",
      "Epoch 605, Loss: 0.4929\n",
      "Epoch 610, Loss: 0.4880\n",
      "Epoch 615, Loss: 0.4832\n",
      "Epoch 620, Loss: 0.4784\n",
      "Epoch 625, Loss: 0.4736\n",
      "Epoch 630, Loss: 0.4689\n",
      "Epoch 635, Loss: 0.4642\n",
      "Epoch 640, Loss: 0.4596\n",
      "Epoch 645, Loss: 0.4550\n",
      "Epoch 650, Loss: 0.4505\n",
      "Epoch 655, Loss: 0.4460\n",
      "Epoch 660, Loss: 0.4415\n",
      "Epoch 665, Loss: 0.4371\n",
      "Epoch 670, Loss: 0.4328\n",
      "Epoch 675, Loss: 0.4285\n",
      "Epoch 680, Loss: 0.4242\n",
      "Epoch 685, Loss: 0.4200\n",
      "Epoch 690, Loss: 0.4158\n",
      "Epoch 695, Loss: 0.4117\n",
      "Epoch 700, Loss: 0.4076\n",
      "Epoch 705, Loss: 0.4035\n",
      "Epoch 710, Loss: 0.3995\n",
      "Epoch 715, Loss: 0.3955\n",
      "Epoch 720, Loss: 0.3916\n",
      "Epoch 725, Loss: 0.3877\n",
      "Epoch 730, Loss: 0.3838\n",
      "Epoch 735, Loss: 0.3800\n",
      "Epoch 740, Loss: 0.3762\n",
      "Epoch 745, Loss: 0.3724\n",
      "Epoch 750, Loss: 0.3687\n",
      "Epoch 755, Loss: 0.3650\n",
      "Epoch 760, Loss: 0.3614\n",
      "Epoch 765, Loss: 0.3578\n",
      "Epoch 770, Loss: 0.3542\n",
      "Epoch 775, Loss: 0.3507\n",
      "Epoch 780, Loss: 0.3472\n",
      "Epoch 785, Loss: 0.3437\n",
      "Epoch 790, Loss: 0.3403\n",
      "Epoch 795, Loss: 0.3369\n",
      "Epoch 800, Loss: 0.3336\n",
      "Epoch 805, Loss: 0.3302\n",
      "Epoch 810, Loss: 0.3270\n",
      "Epoch 815, Loss: 0.3237\n",
      "Epoch 820, Loss: 0.3205\n",
      "Epoch 825, Loss: 0.3173\n",
      "Epoch 830, Loss: 0.3141\n",
      "Epoch 835, Loss: 0.3110\n",
      "Epoch 840, Loss: 0.3079\n",
      "Epoch 845, Loss: 0.3048\n",
      "Epoch 850, Loss: 0.3018\n",
      "Epoch 855, Loss: 0.2988\n",
      "Epoch 860, Loss: 0.2958\n",
      "Epoch 865, Loss: 0.2928\n",
      "Epoch 870, Loss: 0.2899\n",
      "Epoch 875, Loss: 0.2870\n",
      "Epoch 880, Loss: 0.2842\n",
      "Epoch 885, Loss: 0.2813\n",
      "Epoch 890, Loss: 0.2785\n",
      "Epoch 895, Loss: 0.2757\n",
      "Epoch 900, Loss: 0.2730\n",
      "Epoch 905, Loss: 0.2703\n",
      "Epoch 910, Loss: 0.2676\n",
      "Epoch 915, Loss: 0.2649\n",
      "Epoch 920, Loss: 0.2623\n",
      "Epoch 925, Loss: 0.2596\n",
      "Epoch 930, Loss: 0.2571\n",
      "Epoch 935, Loss: 0.2545\n",
      "Epoch 940, Loss: 0.2520\n",
      "Epoch 945, Loss: 0.2494\n",
      "Epoch 950, Loss: 0.2470\n",
      "Epoch 955, Loss: 0.2445\n",
      "Epoch 960, Loss: 0.2420\n",
      "Epoch 965, Loss: 0.2396\n",
      "Epoch 970, Loss: 0.2372\n",
      "Epoch 975, Loss: 0.2349\n",
      "Epoch 980, Loss: 0.2325\n",
      "Epoch 985, Loss: 0.2302\n",
      "Epoch 990, Loss: 0.2279\n",
      "Epoch 995, Loss: 0.2256\n",
      "Final Predicted Output:\n",
      " [[0.96469615]\n",
      " [0.62589584]]\n",
      "Actual Output:\n",
      " [[1.52601074]\n",
      " [0.99003225]]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "    hStates, Yout = forward(X)\n",
    "\n",
    "    loss = np.sum((Yout - Y) ** 2) / 2\n",
    "\n",
    "    dw0, dw1, dw2, db1, db2 = back(X, Y, hStates, Yout)\n",
    "\n",
    "    weightUpdates(dw0, dw1, dw2, db1, db2)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "print(\"Final Predicted Output:\\n\", Yout)\n",
    "print(\"Actual Output:\\n\", Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
